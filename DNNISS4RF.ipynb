{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c1c104-e457-4439-b8fa-e902955d9e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090 D\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import deepxde as dde\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from deepxde.nn.pytorch.fnn import FNN\n",
    "from deepxde.data.data import Tuple\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6b7cc-bf94-4a9c-b8a6-ebf3d98c1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:0') \t\t\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad64b2d8-3ca5-4f07-b7b8-4ced86671783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_move_last_row(data):\n",
    "    num_rows = data.shape[0]\n",
    "    if num_rows == 0:\n",
    "        print(\"The array is empty.\")\n",
    "        return data\n",
    "    last_row = data[-1]\n",
    "    if num_rows == 1:\n",
    "        return data\n",
    "    data = np.delete(data, -1, axis=0)\n",
    "    data = np.insert(data, 0, last_row, axis=0)\n",
    "    return data\n",
    "def auto_move_first_row(data):\n",
    "    num_rows = data.shape[0]\n",
    "    if num_rows == 0:\n",
    "        print(\"The array is empty.\")\n",
    "        return data\n",
    "    first_row = data[0]\n",
    "    if num_rows == 1:\n",
    "        return data\n",
    "    data = np.delete(data, 0, axis=0)\n",
    "    data = np.vstack((data, first_row))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507601d6-a782-4d67-bc7c-ae38e95b733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementwise_l2_error(matrix_true, matrix_pred):\n",
    "    diff = matrix_true - matrix_pred\n",
    "    squared_diff = np.square(diff)\n",
    "    l2_error_matrix = np.sqrt(squared_diff)\n",
    "    return l2_error_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92514adb-6be5-47bc-880f-6f7fad17e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transorm(p):  \n",
    "    array = p.values[0::, 0::] \n",
    "    empty = []\n",
    "    for i in range(len(p)):\n",
    "        lst = array[i][0].split(\" \")\n",
    "        new_lst = list(filter(lambda x: x.strip(), lst))\n",
    "        new_lst2 = [float(x) for x in new_lst]\n",
    "        new_lst2 = np.array(new_lst2, dtype='float32')\n",
    "        empty.append(new_lst2)\n",
    "    empty = np.array(empty, dtype='float32')\n",
    "    scaler = StandardScaler()\n",
    "    empty = scaler.fit_transform(empty)   \n",
    "    return empty, scaler\n",
    "def data_transorm_1(p): \n",
    "    array = p.values[0::, 0::]  \n",
    "    empty = []\n",
    "    for i in range(len(p)):\n",
    "        lst = array[i][0].split(\" \")\n",
    "        new_lst = list(filter(lambda x: x.strip(), lst))\n",
    "        new_lst2 = [float(x) for x in new_lst]\n",
    "        new_lst2 = np.array(new_lst2, dtype='float32')\n",
    "        empty.append(new_lst2)\n",
    "    empty = np.array(empty, dtype='float32')\n",
    "    return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb22574-b2bd-4b2e-af1c-c047248b16d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已读取： V_400\n",
      "已读取： V_420\n",
      "已读取： V_440\n",
      "已读取： V_460\n",
      "已读取： V_480\n",
      "已读取： V_500\n",
      "已读取： V_520\n",
      "已读取： V_540\n",
      "已读取： V_560\n",
      "已读取： V_580\n",
      "已读取： V_600\n",
      "Compiling model...\n",
      "'compile' took 0.547743 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DatasetsConstruct_IC:\n",
    "    def for_train(self, num, start):\n",
    "        self.num = num\n",
    "        self.start = start\n",
    "        for i in range(self.num):\n",
    "            num1 = str(self.start + 20 * i)\n",
    "            E_train = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"E_train.csv\",encoding='gbk', header=None)\n",
    "            E_train = data_transorm_1(E_train)\n",
    "            E_train_IC = E_train[0].reshape(-1,1)\n",
    "\n",
    "            E_test = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"E_test.csv\",encoding='gbk', header=None)\n",
    "            E_test = data_transorm_1(E_test)\n",
    "            E_test_IC = E_test[0].reshape(-1,1)\n",
    "\n",
    "            num = np.linspace(start=1, stop=200, num=200, dtype='float32').reshape(-1, 1)\n",
    "            voltage = np.full(200, self.start + 20 * i, dtype='float32').reshape(-1, 1)\n",
    "            input_train = np.hstack((num, voltage))\n",
    "\n",
    "            print(\"已读取：\", \"V_\" + num1)\n",
    "            if i == 0:\n",
    "                temp_num = num\n",
    "                temp_voltage = voltage\n",
    "                temp_E_train = E_train_IC\n",
    "                temp_E_test = E_test_IC\n",
    "                temp_input_train = input_train\n",
    "            else:\n",
    "                temp_num = np.concatenate((temp_num, num), axis=0)\n",
    "                temp_voltage = np.concatenate((temp_voltage, voltage), axis=0)\n",
    "                temp_E_train = np.concatenate((temp_E_train, E_train_IC), axis=0)\n",
    "                temp_E_test = np.concatenate((temp_E_test, E_test_IC), axis=0)\n",
    "                temp_input_train = np.concatenate((temp_input_train, input_train), axis=0)\n",
    "        x_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "        y_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "        self.x_scaler = x_scaler\n",
    "        self.y_scaler = y_scaler\n",
    "        num = temp_num\n",
    "        voltage = temp_voltage\n",
    "        E_train = temp_E_train\n",
    "        E_train=x_scaler.fit_transform(E_train)\n",
    "        E_test = temp_E_test\n",
    "        E_test=x_scaler.transform(E_test)\n",
    "        input_train = temp_input_train\n",
    "        input_train=y_scaler.fit_transform(input_train)\n",
    "\n",
    "        data_train = Tuple(input_train, E_train, input_train, E_test)\n",
    "\n",
    "        U_col = np.size(input_train, 1)  # 计算 X 的列数\n",
    "        branch_net_input = U_col\n",
    "\n",
    "        return data_train\n",
    "\n",
    "    def restore(self, ic):\n",
    "        ic = self.x_scaler.inverse_transform(ic)\n",
    "        return ic\n",
    "dataset_IC = DatasetsConstruct_IC()\n",
    "data_train_IC= dataset_IC.for_train(num=11,start=400)\n",
    "net_IC = FNN(\n",
    "    [2,300, 300,300, 300,1],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\"\n",
    ")\n",
    "model_IC = dde.Model(data_train_IC, net_IC)\n",
    "model_IC.compile(\"adam\", lr=0.0005, metrics=[\"mean l2 relative error\"])\n",
    "model_IC.restore('H:\\\\DNNISS4RF\\\\model\\\\IC.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d842731-8fa8-4e63-91ec-56b7f38f6f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已读取： V_400\n",
      "已读取： V_420\n",
      "已读取： V_440\n",
      "已读取： V_460\n",
      "已读取： V_480\n",
      "已读取： V_500\n",
      "已读取： V_520\n",
      "已读取： V_540\n",
      "已读取： V_560\n",
      "已读取： V_580\n",
      "已读取： V_600\n"
     ]
    }
   ],
   "source": [
    "def DatasetsConstruct_4train(num, start):\n",
    "    for i in range(num):\n",
    "        num1 = str(start + 20 * i)\n",
    "\n",
    "        Ne_train = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Ne_train.csv\",encoding='gbk', header=None)\n",
    "        Ne_train, temp = data_transorm(Ne_train)\n",
    "\n",
    "        Ne_test = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Ne_test.csv\",encoding='gbk', header=None)\n",
    "        Ne_test, temp = data_transorm(Ne_test)\n",
    "\n",
    "        Te_train = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Te_train.csv\",encoding='gbk', header=None)\n",
    "        Te_train, temp = data_transorm(Te_train)\n",
    "        \n",
    "        Te_test = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Te_test.csv\",encoding='gbk', header=None)\n",
    "        Te_test, temp = data_transorm(Te_test)\n",
    "\n",
    "        Nion_train = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Nion_train.csv\",encoding='gbk', header=None)\n",
    "        Nion_train, temp = data_transorm(Nion_train)\n",
    "\n",
    "        Nion_test = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Nion_test.csv\",encoding='gbk', header=None)\n",
    "        Nion_test, temp = data_transorm(Nion_test)\n",
    "\n",
    "        Je_train = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Je_train.csv\",encoding='gbk', header=None)\n",
    "        Je_train, temp = data_transorm(Je_train)\n",
    "\n",
    "        Je_test = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Je_test.csv\",encoding='gbk', header=None)\n",
    "        Je_test, temp = data_transorm(Je_test)\n",
    "\n",
    "        Ji_train = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Ji_train.csv\",encoding='gbk', header=None)\n",
    "        Ji_train, temp = data_transorm(Ji_train)\n",
    "        \n",
    "        Ji_test = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"Ji_test.csv\",encoding='gbk', header=None)\n",
    "        Ji_test, temp = data_transorm(Ji_test)\n",
    "\n",
    "        E_train = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"E_train.csv\",encoding='gbk', header=None)\n",
    "        E_train, temp = data_transorm(E_train)\n",
    "\n",
    "        E_test = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num1 + \"\\\\\" + \"E_test.csv\",encoding='gbk', header=None)\n",
    "        E_test, temp = data_transorm(E_test)\n",
    "\n",
    "        print(\"已读取：\", \"V_\" + num1)\n",
    "        if i == 0:\n",
    "            temp_Ne_train = Ne_train\n",
    "            temp_Ne_test = Ne_test\n",
    "            temp_Te_train = Te_train\n",
    "            temp_Te_test = Te_test\n",
    "            temp_Nion_train = Nion_train\n",
    "            temp_Nion_test = Nion_test\n",
    "            temp_Je_train = Je_train\n",
    "            temp_Je_test = Je_test\n",
    "            temp_Ji_train = Ji_train\n",
    "            temp_Ji_test = Ji_test\n",
    "            temp_E_train = E_train\n",
    "            temp_E_test = E_test\n",
    "\n",
    "        else:\n",
    "            temp_Ne_train = np.concatenate((temp_Ne_train, Ne_train), axis=0)\n",
    "            temp_Ne_test = np.concatenate((temp_Ne_test, Ne_test), axis=0)\n",
    "            temp_Nion_train = np.concatenate((temp_Nion_train, Nion_train), axis=0)\n",
    "            temp_Nion_test = np.concatenate((temp_Nion_test, Nion_test), axis=0)\n",
    "            temp_Te_train = np.concatenate((temp_Te_train, Te_train), axis=0)\n",
    "            temp_Te_test = np.concatenate((temp_Te_test, Te_test), axis=0)\n",
    "            temp_Je_train = np.concatenate((temp_Je_train, Je_train), axis=0)\n",
    "            temp_Je_test = np.concatenate((temp_Je_test, Je_test), axis=0)\n",
    "            temp_Ji_train = np.concatenate((temp_Ji_train, Ji_train), axis=0)\n",
    "            temp_Ji_test = np.concatenate((temp_Ji_test, Ji_test), axis=0)\n",
    "            temp_E_train = np.concatenate((temp_E_train, E_train), axis=0)\n",
    "            temp_E_test = np.concatenate((temp_E_test, E_test), axis=0)\n",
    "\n",
    "    Ne_train = temp_Ne_train\n",
    "    Ne_test = temp_Ne_test\n",
    "    Te_train = temp_Te_train\n",
    "    Te_test = temp_Te_test\n",
    "    Nion_train = temp_Nion_train\n",
    "    Nion_test = temp_Nion_test\n",
    "    Je_train = temp_Je_train\n",
    "    Je_test = temp_Je_test\n",
    "    Ji_train = temp_Ji_train\n",
    "    Ji_test = temp_Ji_test\n",
    "    E_train = temp_E_train\n",
    "    E_test = temp_E_test\n",
    "\n",
    "    dataset_Ne2Te = Tuple(Ne_train, Te_train, Ne_test, Te_test)\n",
    "\n",
    "    dataset_Ji2Ni = Tuple(Ji_train, Nion_train, Ji_test, Nion_test)\n",
    "\n",
    "    dataset_je2Ne = Tuple(Je_train, Ne_train, Je_test, Ne_test)\n",
    "\n",
    "    dataset_E2ji = Tuple(E_train, Ji_train, E_test, Ji_test)\n",
    "\n",
    "    dataset_E2je = Tuple(E_train, Je_train, E_test, Je_test)\n",
    "\n",
    "    dataset_Nei2E = Tuple(Ne_train, E_train, Ne_test, E_test)\n",
    "\n",
    "    return dataset_Ne2Te, dataset_Ji2Ni, dataset_je2Ne, dataset_E2ji, dataset_E2je, dataset_Nei2E\n",
    "\n",
    "data_train_Ne2Te, data_train_Ji2Ni, data_train_je2Ne, data_train_E2ji, data_train_E2je, data_train_Nei2E = DatasetsConstruct_4train(num=11,start=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a7e6d75-a135-4476-b038-5a5ecf550408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "'compile' took 0.000353 s\n",
      "\n",
      "Compiling model...\n",
      "'compile' took 0.000280 s\n",
      "\n",
      "Compiling model...\n",
      "'compile' took 0.000237 s\n",
      "\n",
      "Compiling model...\n",
      "'compile' took 0.000243 s\n",
      "\n",
      "Compiling model...\n",
      "'compile' took 0.000271 s\n",
      "\n",
      "Compiling model...\n",
      "'compile' took 0.000236 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_Nei2E = FNN(\n",
    "    [200,300, 400,400, 300,200],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\"\n",
    ")\n",
    "model_Nei2E = dde.Model(data_train_Nei2E, net_Nei2E)\n",
    "model_Nei2E.compile(\"adam\", lr=0.0005, metrics=[\"mean l2 relative error\"])\n",
    "model_Nei2E.restore('H:\\\\DNNISS4RF\\\\model\\\\Nei2E.pt')\n",
    "\n",
    "net_E2je = FNN(\n",
    "    [200,300, 400,400, 300,200],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\"\n",
    ")\n",
    "model_E2je = dde.Model(data_train_E2je, net_E2je)\n",
    "model_E2je.compile(\"adam\", lr=0.0005, metrics=[\"mean l2 relative error\"])\n",
    "model_E2je.restore('H:\\\\DNNISS4RF\\\\model\\\\E2je.pt')\n",
    "\n",
    "net_E2ji = FNN(\n",
    "    [200,300, 400,400, 300,200],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\"\n",
    ")\n",
    "model_E2ji = dde.Model(data_train_E2ji, net_E2ji)\n",
    "model_E2ji.compile(\"adam\", lr=0.0005, metrics=[\"mean l2 relative error\"])\n",
    "model_E2ji.restore('H:\\\\DNNISS4RF\\\\model\\\\E2ji.pt')\n",
    "\n",
    "net_je2Ne = FNN(\n",
    "    [200,300, 400,400, 300,200],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\"\n",
    ")\n",
    "model_je2Ne = dde.Model(data_train_je2Ne, net_je2Ne)\n",
    "model_je2Ne.compile(\"adam\", lr=0.0005, metrics=[\"mean l2 relative error\"])\n",
    "model_je2Ne.restore('H:\\\\DNNISS4RF\\\\model\\\\Je2Ne.pt')\n",
    "\n",
    "net_Ji2Ni = FNN(\n",
    "    [200,300, 400,400, 300,200],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\"\n",
    ")\n",
    "model_Ji2Ni = dde.Model(data_train_Ji2Ni, net_Ji2Ni)\n",
    "model_Ji2Ni.compile(\"adam\", lr=0.0005, metrics=[\"mean l2 relative error\"])\n",
    "model_Ji2Ni.restore('H:\\\\DNNISS4RF\\\\model\\\\Ji2Ni.pt')\n",
    "\n",
    "net_Ne2Te = FNN(\n",
    "    [200,300, 400,400, 300,200],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\"\n",
    ")\n",
    "model_Ne2Te = dde.Model(data_train_Ne2Te, net_Ne2Te)\n",
    "model_Ne2Te.compile(\"adam\", lr=0.0005, metrics=[\"mean l2 relative error\"])\n",
    "model_Ne2Te.restore('H:\\\\DNNISS4RF\\\\model\\\\Ne2Te.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d816b87-a130-4da5-a2f0-27d74a114bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNNISS(voltage):\n",
    "    num2 = str(voltage)\n",
    "\n",
    "    Ne_verify = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num2 + \"\\\\\" + \"Ne_train.csv\",encoding='gbk', header=None)\n",
    "    Ne_verify1, Ne_scaler  = data_transorm(Ne_verify)\n",
    "    Ne_verify = Ne_scaler.inverse_transform(Ne_verify1)\n",
    "\n",
    "    Nion_verify = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num2 + \"\\\\\" + \"Nion_train.csv\",encoding='gbk', header=None)\n",
    "    Nion_verify1, Nion_scaler  = data_transorm(Nion_verify)\n",
    "    Nion_verify = Nion_scaler.inverse_transform(Nion_verify1)\n",
    "    \n",
    "    E_verify = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num2 + \"\\\\\" + \"E_train.csv\",encoding='gbk', header=None)\n",
    "    E_verify1, E_scaler  = data_transorm(E_verify)\n",
    "    E_verify = E_scaler.inverse_transform(E_verify1)\n",
    "    \n",
    "    Je_verify = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num2 + \"\\\\\" + \"Je_train.csv\",encoding='gbk', header=None)\n",
    "    Je_verify1, Je_scaler = data_transorm(Je_verify)\n",
    "    Je_verify = Je_scaler.inverse_transform(Je_verify1)\n",
    "    \n",
    "    Ji_verify = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num2 + \"\\\\\" + \"Ji_train.csv\",encoding='gbk', header=None)\n",
    "    Ji_verify1, Ji_scaler = data_transorm(Ji_verify)\n",
    "    Ji_verify = Ji_scaler.inverse_transform(Ji_verify1)\n",
    "\n",
    "    Te_verify = pd.read_csv(\"H:\\\\DNNISS4RF\\\\datasets\\\\\" + \"V_\" + num2 + \"\\\\\" + \"Te_train.csv\",encoding='gbk', header=None)\n",
    "    Te_verify1, Te_scaler = data_transorm(Te_verify)\n",
    "    Te_verify = Te_scaler.inverse_transform(Te_verify1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    E_pred1 = []\n",
    "    Je_pred1 = []\n",
    "    Ji_pred1 = []\n",
    "    Ne_pred1 = []\n",
    "    Nion_pred1 = []\n",
    "    Te_pred1 = []\n",
    "\n",
    "    ic_transform = np.array([[1,400],[200,600]], dtype='float32')\n",
    "    ic_scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    ic_transform = ic_scaler.fit_transform(ic_transform)\n",
    "    num_validate = np.linspace(start=1, stop=200, num=200, dtype='float32').reshape(-1, 1)\n",
    "    voltage_validate = np.full(200, voltage, dtype='float32').reshape(-1, 1)\n",
    "    ic_input = np.hstack((num_validate, voltage_validate))\n",
    "    ic_input = ic_scaler.transform(ic_input)\n",
    "    ic_pred = model_IC.predict(ic_input)\n",
    "    ic_pred = dataset_IC.restore(ic_pred)\n",
    "    E_ic = E_scaler.transform(ic_pred.reshape(1, -1))\n",
    "    \n",
    "    for i in range(np.size(Ne_verify1, 0)):\n",
    "        E_input = E_ic.astype(np.float32)\n",
    "        Je_pred = model_E2je.predict(E_input)\n",
    "        Ji_pred = model_E2ji.predict(E_input)\n",
    "        Je_pred1.append(Je_pred)\n",
    "        Ji_pred1.append(Ji_pred)\n",
    "        Je_input = Je_pred.astype(np.float32)\n",
    "        Ji_input = Ji_pred.astype(np.float32)\n",
    "        Ne_pred = model_je2Ne.predict(Je_input)\n",
    "        Ni_pred = model_Ji2Ni.predict(Ji_input)\n",
    "        Ne_pred1.append(Ne_pred)\n",
    "        Nion_pred1.append(Ni_pred)\n",
    "        Te_input = Ne_pred.astype(np.float32)\n",
    "        Te_pred = model_Ne2Te.predict(Te_input)\n",
    "        Te_pred1.append(Te_pred)\n",
    "        Nei_input = Ne_pred\n",
    "        E_pred = model_Nei2E.predict(Nei_input)\n",
    "        E_pred1.append(E_pred)  \n",
    "        E_ic = E_pred\n",
    " \n",
    "    end_time = time.time()\n",
    "    time_difference = end_time - start_time\n",
    "    print('预测耗时：',time_difference,'s')\n",
    "     \n",
    "    E_pred1 = np.array(E_pred1)\n",
    "    E_pred1 = np.squeeze(E_pred1)\n",
    "    E_pred1 = auto_move_last_row(E_pred1)\n",
    "    E_pred = E_scaler.inverse_transform(E_pred1)\n",
    "    \n",
    "    Je_pred1 = np.array(Je_pred1)\n",
    "    Je_pred1 = np.squeeze(Je_pred1)\n",
    "    Je_pred = Je_scaler.inverse_transform(Je_pred1)\n",
    "    \n",
    "    Ji_pred1 = np.array(Ji_pred1)\n",
    "    Ji_pred1 = np.squeeze(Ji_pred1)\n",
    "    Ji_pred = Ji_scaler.inverse_transform(Ji_pred1)\n",
    "    \n",
    "    Ne_pred1 = np.array(Ne_pred1)\n",
    "    Ne_pred1 = np.squeeze(Ne_pred1)\n",
    "    Ne_pred = Ne_scaler.inverse_transform(Ne_pred1)\n",
    "    \n",
    "    Nion_pred1 = np.array(Nion_pred1)\n",
    "    Nion_pred1 = np.squeeze(Nion_pred1)\n",
    "    Nion_pred = Nion_scaler.inverse_transform(Nion_pred1)\n",
    "\n",
    "    Te_pred1 = np.array(Te_pred1)\n",
    "    Te_pred1 = np.squeeze(Te_pred1)\n",
    "    Te_pred = Te_scaler.inverse_transform(Te_pred1)\n",
    "    \n",
    "    df1 = pd.DataFrame(data=Ne_pred)\n",
    "    df1.to_csv('H:\\\\DNNISS4RF\\\\pred\\\\'+'V_'+num2+' Ne_pred.csv', index=False, header=False,mode='w')\n",
    "    df2 = pd.DataFrame(data=E_pred)\n",
    "    df2.to_csv('H:\\\\DNNISS4RF\\\\pred\\\\'+'V_'+num2+' E_pred.csv', index=False, header=False,mode='w')\n",
    "    df3 = pd.DataFrame(data=Je_pred)\n",
    "    df3.to_csv('H:\\\\DNNISS4RF\\\\pred\\\\'+'V_'+num2+' Je_pred.csv', index=False, header=False,mode='w')\n",
    "    df4 = pd.DataFrame(data=Ji_pred)\n",
    "    df4.to_csv('H:\\\\DNNISS4RF\\\\pred\\\\'+'V_'+num2+' Ji_pred.csv', index=False, header=False,mode='w')\n",
    "    df5 = pd.DataFrame(data=Nion_pred)\n",
    "    df5.to_csv('H:\\\\DNNISS4RF\\\\pred\\\\'+'V_'+num2+' Nion_pred.csv', index=False, header=False,mode='w')\n",
    "    df6 = pd.DataFrame(data=Te_pred)\n",
    "    df6.to_csv('H:\\\\DNNISS4RF\\\\pred\\\\'+'V_'+num2+' Te_pred.csv', index=False, header=False,mode='w')\n",
    "\n",
    "    #error\n",
    "    Ne_error = dde.metrics.l2_relative_error(Ne_verify, Ne_pred)\n",
    "    Ni_error = dde.metrics.l2_relative_error(Nion_verify, Nion_pred)\n",
    "    E_error = dde.metrics.l2_relative_error(E_verify, E_pred)\n",
    "    Je_error = dde.metrics.l2_relative_error(Je_verify, Je_pred)\n",
    "    Ji_error = dde.metrics.l2_relative_error(Ji_verify, Ji_pred)\n",
    "    Te_error = dde.metrics.l2_relative_error(Te_verify, Te_pred)\n",
    "\n",
    "    print('Ne_error',Ne_error)\n",
    "    print('Ni_error',Ni_error)\n",
    "    print('E_error',E_error)\n",
    "    print('Je_error',Je_error)\n",
    "    print('Ji_error',Ji_error)\n",
    "    print('Te_error',Te_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d6e037-34de-4922-a92a-46f1af56e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测耗时： 0.5590000152587891 s\n",
      "Ne_error 0.00031201637\n",
      "Ni_error 1.1220948e-05\n",
      "E_error 0.0015289898\n",
      "Je_error 0.0\n",
      "Ji_error 0.00168442\n",
      "Te_error 0.00060871453\n"
     ]
    }
   ],
   "source": [
    "DNNISS(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python]",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
